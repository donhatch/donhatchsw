/* vim: set filetype=java: */
// Author: Don Hatch (hatch@hadron.org)
#include "macros.h"

package com.donhatchsw.util;

/**
*
* Robust implementations of hyperbolic and inverse hyperbolic trig functions,
* and expm1 and log1p,
* and hypot; all stuff that should be in the standard math library.
* <br>
* For explanations of some of the robust algorithms used, see:
* <a href="http://plunk.org/~hatch/rightway.php">http://plunk.org/~hatch/rightway.php</a>
* <br>
* TODO: more stuff to add: acosf1 (inverse of cosf1? or 1-acos(f)? get it straight, sincm1, asinh1p, sinhc, sinhcm1, asinhc1p... get it all straight.)
* <br>
* TODO: higher order stuff... is there a methodology? e.g. exp(x) -> exp(x)-1 -> (exp(x)-1)/x -> (exp(x)-1)/x-1 -> ((exp(x)-1.)/x-1.)/(x/2.) -> ((exp(x)-1.)/x-1.)/(x/2.)-1. -> (((exp(x)-1.)/x-1.)/(x/2.)-1.)/(x/3.) -> (((exp(x)-1.)/x-1.)/(x/2.)-1.)/(x/3.)-1. -> ((((exp(x)-1.)/x-1.)/(x/2.)-1.)/(x/3.)-1.)/(x/4.)
* <br>
* TODO: confidence tests actually verifying accuracy of some of these in the problem cases, e.g. my acosh wasn't that good and the new one is great
* <br>
* TODO: implement asinh_difference at least (needed for Catenary)
* TODO: change *_difference functions to *_difference_quotient
* TODO: implement various *_difference_integral functions? I think. Maybe.
*/

public class MyMath
{
    private MyMath() {} // non-instantiatable

    /**
    * exp(x)-1, accurate even when x is small.
    */
   /* This was found in:
    * <a href="http://www.cs.berkeley.edu/~wkahan/Math128/Sumnfp.pdf">
    * http://www.cs.berkeley.edu/~wkahan/Math128/Sumnfp.pdf</a>
    * <br>
    * Achieves "nearly full working relative accuracy despite cancellation
    * no matter how tiny x may be".
    */
    public static double
    expm1(double x)
    {
        double u = Math.exp(x);
        if (u == Double.POSITIVE_INFINITY)
            return Double.POSITIVE_INFINITY;
        if (u == 1.)
            return x;
        if (u-1. == -1.)
            return -1.;
        return (u-1.)*x/Math.log(u);
    }

    /**
    * (exp(x)-1)/x, accurate even when x is small.
    */
    public static double
    expm1_over_x(double x)
    {
        double u = Math.exp(x);
        if (u == Double.POSITIVE_INFINITY)
            return Double.POSITIVE_INFINITY;
        if (u == 1.)
            return 1.;
        if (u-1. == -1.)
            return -1./x;
        return (u-1.)/Math.log(u);
    }

    /**
    *  log(1+x), accurate even when x is small.
    */
    /*
    *  Found in gsl (gnu scientific library),
    *  distributed under GPL version 2,
    *  but I'm sure it's from Kahan too.
    */
    public static double
    log1p(double x)
    {
        if (x == -1.)
            return Double.NEGATIVE_INFINITY; // grr special case since apparently -inf + nan == nan (I'd prefer -inf)
        if (x == Double.POSITIVE_INFINITY)
            return Double.POSITIVE_INFINITY;
        double u = 1.+x;
        if (u == Double.POSITIVE_INFINITY) // grr, special case for max double
            return Math.log(x);
        // yow! see below, lots simpler I think
        return Math.log(u) - ((u-1.)-x)/u; // cancels errors with IEEE arithmetic
    }

    /*
        XXX the following might be preferable...
 from http://www.hursley.ibm.com/majordomo/JSR-DECIMAL/archives/jsr-decimal.archive.0102/Author/article-6.html:

It is not unheard of for math libraries to deliberately add quantities
that might have very different magnitudes to see what sort of roundoff
occurs.  For example, take Kahan's log1p implementation (log1p is the
result of ln(1 + x) -- because of roundoff in a given precision,
having a separate function is more accurate than the obvious way to
compute this function):

log1p(x) {
  if ( (1.0 + x ) == 1.0)
    return x;
  else
    return log(1.0 +x)*x/(1.0 +x);     XXX that doesn't look right to me... should be dividing by something close to x, not close to 1+x. good thing I'm not using this.
}

The ( (1.0 + x ) == 1.0) expression computes the sum of x and 1.0
simply to see of x gets rounded away.  In this case, computing the
exact value of (1.0 + x) could seriously effect the performance of
this function when it should be very fast.
    XXX and this, from
        http://kristopherjohnson.net/twiki/pub/Main/MathLibFunctions/alg.html

 * Note: Assuming log() return accurate answer, the following
 *       algorithm can be used to compute log1p(x) to within a few ULP:
 *
 *              u = 1+x;
 *              if(u==1.0) return x ; else
 *                         return log(u)*(x/(u-1.0));
 *
 *       See HP-15C Advanced Functions Handbook, p.193.
 *

    XXX whoa, and lots of other good stuff at:
http://kristopherjohnson.net/twiki/pub/Main/MathLibFunctions/alg.html

* Also see this stackoverflow question which asks for merits of yet two more methods:
*   http://stackoverflow.com/questions/24134816/goldbergs-log1p-vs-gsl-log1p

    */

    /**
    *  log(1+x)/x, accurate even when x is small.
    */
    public static double
    log1p_over_x(double x)
    {
        double u = 1. + x;
        if (u == 1.)
            return 1.;
        return Math.log(u)/(u-1.);
    }


    /** exp(X+x)-exp(X), accurate even when x is small. */
    public static double
    exp_difference(double X, double x)
    {
        // exp(X+x)-exp(X) = exp(X)*exp(x) - exp(X)
        //                 = exp(X)*(exp(x)-1)
        //                 = exp(X)*expm1(x)
        return expm1(x) * Math.exp(X);
    }
    /** log(X+x)-log(X), accurate even when x is small. */
    public static double
    log_difference(double X, double x)
    {
        // log(X+x)-log(X) = log((X+x)/X)
        //                 = log(1+x/X)
        //                 = log1p(x/X)
        // XXX decide whether the following is relevant: http://stackoverflow.com/questions/29827309/compute-logx-y-given-logx-and-logy-without-overflows
        return log1p(x/X);
    }
    /** sqrt(X+x)-sqrt(X), accurate even when x is small. */
    public static double
    sqrt_difference(double X, double x)
    {
        if (X==0. && x==0.) return 0.; // XXX not sure this is right! think about it
        // sqrt(X+x)-sqrt(X) = (sqrt(X+x)-sqrt(X))*(sqrt(X+x)+sqrt(X))/(sqrt(X+x)+sqrt(X))
        //                   = (X+x-X) / (sqrt(X+x) + sqrt(X))
        //                   = x / (sqrt(X+x) + sqrt(X))
        return x / (Math.sqrt(X+x) + Math.sqrt(X));
    }
    /** (X+x)^2-X^2, accurate even when x is small. */
    public static double
    sqr_difference(double X, double x)
    {
        // (X+x)^2 - X^2 = 2*X*x + x^2 = x*(2*X+x)
        return x * (2*X + x);
    }
    /** (X+x)^3-X^3, accurate even when x is small. */
    public static double
    cub_difference(double X, double x)
    {
        // (X+x)^3 - X^3 = 3*X^2*x + 3*X*x^2 + x^3 = x*(3*X^2 + 3*X*x + x^2)
        // XXX are any orderings preferable to others?
        return x * (3*(X*X) + 3*(X*x) + x*x);
    }

    /** cos(T+x)-cos(T), accurate even when x is small. */
    public static double
    cos_difference(double T, double t)
    {
        // cos(T+t)-cos(t) = cos(t)*cos(T) - sin(t)*sin(T) - cos(T) according to wolframalpha
        //                 = cos(T)*(cos(t)-1) - sin(t)*sin(T)
        //                 = -cosf1(t)*cos(T) - sin(t)*sin(T)
        return -cosf1(t)*Math.cos(T) - Math.sin(t)*Math.sin(T);
    }
    /** sin(T+t)-sin(T), accurate even when t is small. */
    public static double
    sin_difference(double T, double t)
    {
        // sin(T+t)-cos(t) = sin(t)*cos(T) + cos(t)*sin(T) - sin(T) according to wolframalpha
        //                 = sin(t)*cos(T) + (cos(t)-1)*sin(T)
        //                 = sin(t)*cos(T) - cosf1(t)*sin(T)
        return Math.sin(t)*Math.cos(T) - cosf1(t)*Math.sin(T);
    }
    /** tan(T+t)-tan(T), accurate even when t is small. */
    public static double
    tan_difference(double T, double t)
    {
        System.err.println("WARNING: tan_difference implemented naively");
        return Math.tan(T+t) - Math.tan(T);
    }
    /** cosh(X+x)-cosh(X), accurate even when x is small; X may be infinite. */
    public static double
    cosh_difference(double Xnumerator, double Xdenominator, double x)
    {
        // cosh(X+x)-cosh(X) = (exp(X+x)+exp(-(X+x)))/2 - (exp(X)+exp(-X))/2
        //                   = (exp(X+x)-exp(X))/2 + (exp(-X-x)-exp(-X))/2
        //                   = (exp_difference(X,x) + exp_difference(-X,-x)) / 2
        //                   = expm1(x)*exp(X) + expm1(-x)*exp(-X)
        if (Math.abs(Xnumerator) <= Math.abs(Xdenominator))
        {
            // mostly vertical case
            double X = Xnumerator/Xdenominator; // stable in this case
            return exp_difference(X, x)*.5 + exp_difference(-X, -x)*.5; // ordered to prevent overflow
        }
        else
        {
            // mostly horizontal case
            System.err.println("WARNING: cosh_difference implemented naively in mostly horizontal case");
            return Math.cosh(Xnumerator/Xdenominator+x) - Math.cosh(Xnumerator/Xdenominator);
        }
    }
    /** sinh(X+x)-sinh(X), accurate even when x is small; X may be infinite. */
    public static double
    sinh_difference(double Xnumerator, double Xdenominator, double x)
    {
        // sinh(X+x)-sinh(X) = (exp(X+x)-exp(-(X+x)))/2 - (exp(X)-exp(-X))/2
        //                   = (exp(X+x)-exp(X))/2 - (exp(-X-x)-exp(-X))/2
        //                   = (exp_difference(X,x) - exp_difference(-X,-x)) / 2
        //                   = expm1(x)*exp(X) - expm1(-x)*exp(-X)
        if (Math.abs(Xnumerator) <= Math.abs(Xdenominator))
        {
            double X = Xnumerator/Xdenominator; // stable in this case
            return exp_difference(X, x)*.5 - exp_difference(-X, -x)*.5; // ordered to prevent overflow
        }
        else
        {
            // mostly horizontal case
            System.err.println("WARNING: sinh_difference implemented naively in mostly horizontal case");
            return Math.sinh(Xnumerator/Xdenominator+x) - Math.sinh(Xnumerator/Xdenominator);
        }
    }
    /** tanh(X+x)-tanh(X), accurate even when x is small; X may be infinite. */
    public static double
    tanh_difference(double Xnumerator, double Xdenominator, double x)
    {
        System.err.println("WARNING: tanh_difference implemented naively");
        double X = Xnumerator / Xdenominator;
        return Math.tanh(X+x) - Math.tanh(X);
    }
    /** sinc(T+t)-sinc(T), accurate even when t is small. */
    public static double
    sinc_difference(double T, double t)
    {
        // TODO: does it make sense for T to be infinite?  Maybe there needs to be more fine-grained params in that case?  (Tnumerator, Tdenominator, T mod 2*pi)
        // sinc(T+t)-sinc(T) = sin(T+t)/(T+t) - sin(T)/T
        //                   = (sin(T+t)*T - sin(T)*(T+t)) / ((T+t)*T)
        //                   = (sin(T+t)*T - sin(T)*T - sin(T)*t) / ((T+t)*T)
        //                   = (sin_difference(T,t)*T - t*sin(T)) / ((T+t)*T)
        //                   = (sin_difference(T,t) - t*sinc(T)) / (T+t)
        // XXX TODO: clearly need to check for 0/0
        if (T+t == 0.) // XXX haven't thought about what this means. seems weird if it's asymmetric... ? i.e. different behavior when T+t from when T==0
            return sin_over_x(T+t) - sin_over_x(T);
        return (sin_difference(T,t) - t*sin_over_x(T)) / (T+t);
    }

    /** acos(X+x)-acos(X), accurate even when x is small. */
    public static double
    acos_difference(double X, double x)
    {
        System.err.println("WARNING: acos_difference implemented naively");
        return Math.acos(X+x) - Math.acos(X);
    }
    /** asin(Y+y)-asin(y), accurate even when y is small. */
    public static double
    asin_difference(double Y, double y)
    {
        System.err.println("WARNING: asin_difference implemented naively");
        return Math.asin(Y+y) - Math.asin(Y);
    }
    /** acosh(Y+y)-acosh(Y), accurate even when y is small; Y may be infinite. */
    public static double
    acosh_difference(double Ynumerator, double Ydenominator, double y)
    {
        System.err.println("WARNING: acosh_difference implemented naively");
        double Y = Ynumerator / Ydenominator;
        return MyMath.acosh(Y+y) - MyMath.acosh(Y);
    }
    /** asinh(T+t)-asinh(T), accurate even when t is small; T may be infinite. */
    public static double
    asinh_difference(double Tnumerator, double Tdenominator, double t)
    {
        //System.err.println("WARNING: asinh_difference implemented naively");
        double T = Tnumerator / Tdenominator;
        return MyMath.asinh(T+t) - MyMath.asinh(T);
    }
    /** atanh(T+t)-atanh(T), accurate even when t is small. */
    public static double
    atanh_difference(double T, double t)
    {
        System.err.println("WARNING: atanh_difference implemented naively");
        return MyMath.atanh(T+t) - MyMath.atanh(T);
    }


    /** hyperbolic sine function */
    public static double
    sinh(double x)
    {
        // sinh(x) = (e^x - e^-x) / 2
        //         = (e^x - 1)(e^x + 1) / e^x / 2;
        //         = expm1(x) * (expm1(x)+2) / (expm1(x)+1) / 2
        // XXX or:  (expm1(x)-expm1(-x))/2 ... is that more accurate?
        if (x >= 0.)
        {
            double u = expm1(x);
            if (u == Double.POSITIVE_INFINITY)
                return Double.POSITIVE_INFINITY;
            return .5 * u / (u+1) * (u+2); // ordered to avoid overflow when big
        }
        else
        {
            double u = expm1(-x);
            if (u == Double.POSITIVE_INFINITY)
                return Double.NEGATIVE_INFINITY;
            return -.5 * u / (u+1) * (u+2); // ordered to avoid overflow when big
        }
    }

    /** hyperbolic cosine function */
    public static double
    cosh(double x)
    {
        // cosh(x) = (e^x + e^-x) / 2
        // I don't think there are any cancellation issues
        // (though probably coshm1, below, is more useful).
        // We start by taking abs(x), just to make sure we get the exact same
        // result for x and -x.
        // XXX should we be doing coshm1(x) + 1? not sure
        double e_x = Math.exp(Math.abs(x));
        return (e_x + 1./e_x) * .5;
    }

    /** cosh(x)-1, accurate even when x is small. */
    public static double
    coshm1(double x)
    {
        if (true)
        {
            // cosh(x) - 1 = (e^x + e^-x) / 2 - 1
            //             = (e^x - 2 + e^-x) / 2
            //             = (e^2x - 2*e^x + 1) / e^x / 2
            //             = (e^x - 1)^2 / e^x / 2
            //             = expm1(x)^2 / (expm1(x)+1) / 2
            double u = expm1(Math.abs(x));
            if (u == Double.POSITIVE_INFINITY)
                return Double.POSITIVE_INFINITY;
            return .5 * u / (u+1) * u; // ordered to avoid overflow when big
        }
        else
        {
            // Seemingly more straightforward,
            // but actually more expensive since it takes two different expm1's.
            // But... should check to see whether it's more accurate?
            // Some discussion on some web site (mentioned elsewhere in this file)
            // indicates it is.
            // cosh(x) - 1 = (e^x + e^-x) / 2 - 1
            //             = (e^x - 1 + e^-x - 1) / 2
            //             = (e^x - 1)/2 + (e^-x - 1)/2
            //             = (expm1(x) + expm1(-x)) / 2
            return .5*expm1(x) + .5*expm1(-x); // ordered to avoid overflow when big
        }
        // TODO: 2*sinh(.5*x)^2 ? yeah I think so.  inverse of what we do with acosh1p below (but not any more)
    }

    /** acosh(1+x), accurate even when x is small. */
    public static double
    acosh1p(double x)
    {
        // previous version-- not as direct, but interesting because it only
        // refers to x once, so it's easily invertible, leading to a formula
        // for the inverse function coshm1 above
        //return 2 * asinh(Math.sqrt(x/2));

        // acosh(X) = log(X + sqrt((X+1)*(X-1))
        // So acosh1p(x) = acosh(1+x)
        //               = log((1+x) + sqrt((1+x)+1)*((1+x)-1))
        //               = log1p(x + sqrt(x*(x+2))
        return log1p(x + Math.sqrt(x * (x+2.)));
    }


    /** hyperbolic tangent function */
    public static double
    tanh(double x)
    {
        // tanh(x) = sinh(x) / cosh(x)
        //         = (e^x - e^-x) / (e^x + e^-x)
        //         = (e^2x - 1) / (e^2x + 1)
        //         = expm1(2*x) / (expm1(2*x) + 2)
        // Note that overflows prematurely and incorrectly returns NaN
        // at +infinity, but it behaves well at -infinity and negative numbers,
        // so for the positive case just use the mirror image.
        if (x > 0.)
        {
            //return -tanh(-x); // works but unnecessary function call
            double u = expm1(2. * -x);
            return -(u / (u + 2.));
        }
        else
        {
            double u = expm1(2.*x);
            return u / (u + 2.);
        }
    }

    /** inverse hyperboiic sine function */
    public static double
    asinh(double x)
    {
        if (x == Double.NEGATIVE_INFINITY)
            return Double.NEGATIVE_INFINITY;
        if (x == Double.POSITIVE_INFINITY)
            return Double.POSITIVE_INFINITY;
        // To make it better behaved near 0:
        // asinh(x) = log(x + sqrt(x^2 + 1))
        //          = log1p(x + sqrt(x^2 + 1) - 1)
        //          = log1p(x + (sqrt(x^2+1)-1)*(sqrt(x^2+1)+1)/(sqrt(x^2+1)+1))
        //          = log1p(x + (x^2+1 - 1)/(sqrt(x^2+1)+1))
        //          = log1p(x + x^2 / (sqrt(x^2+1)+1))
        //          = log1p(x * (1 + x / (sqrt(x^2+1)+1) ))
        // That fixes it near 0:
        //     gnuplot> plot [-1e-15:1e-15] asinh(x), asinh_smart(x)
        // However, for some reason the formula still isn't symmetric,
        // even though the function is symmetric, and furthermore
        // it behaves rather atrociously for large negative arguments:
        //     gnuplot> plot [-1e9:1e9] asinh(x), asinh_smart(x)
        // (where asinh_smart is the above).
        // (Note that gnuplot's is just as bad, and in fact seems to give up fairly early as well... XXX file a bug against gnuplot!)
        // Therefore we special case negative arguments.
        // XXX TODO: might want to switch to more naive log-based when args very large-- might be better behaved? not sure.
        // See https://code.google.com/p/v8/issues/detail?id=3468 and 3 derived bugs for possibly good ideas.
        // Also it looks like the python math library is well-thought-out
        // and probably gets it right:
        //     http://svn.python.org/projects/python/branches/pep-0384/Modules/_math.c
        // And in fact it explains some stuff!
        if (x >= 0)
            return  log1p( x * (1. + x / (hypot(x,1.)+1.)));
        else
            return -log1p(-x * (1. - x / (hypot(x,1.)+1.)));
    }

    /** inverse hyperbolic cosine function */
    public static double
    acosh(double x)
    {
        // Only defined for x >= 1.

        // acosh(x) = log(x + sqrt(x^2 - 1))
        // Use the formula given by Kahan in
        // "Branch Cuts for Complex Elementary Functions",
        // as quoted by Cleve Moler in:
        // http://www.mathworks.com/company/newsletter/clevescorner/sum98cleve.shtml
        //
        // XXX possibly not nearly as good as it could be?
        // see https://code.google.com/p/v8/issues/detail?id=3509
        // it says:
        //      Math.acosh(1+1e-10) -> 0.000014142136208733941
        // the correct answer is 1.4142136208675862d-5.
        // They suggest what fdlibm uses (for x near 1):
        //      log(2x-1/(sqrt(x*x-1)+x))
        // That gives exactly the above answer, but... ?
        // Mine gives:
        //     1.4142136208643407e-05
        // But wolframalpha acosh(1+10^-10) says:
        //     .0000141421356236130993578217
        // HOWEVER... worlframalpha isn't really calculating the same thing.
        // How do I get it to?
        // Well... to start with, in python, let's get a mathematically exact
        // representation of (double)(1+1e-10):
        //      `(1+1e-10 - 1) * (1<<49)`
        //      '56295.0'
        // So that says that (double)(1+1e-10) is exactly 1+56295/2^49.
        // Feeding into wolframalpha:
        //      acosh(1+56295/2^49)
        // It says:
        //      .00001414213620867586 135459
        // Hmph!  So they were right in the bug description :-(
        // Except maybe the last digit?
        //      `.00001414213620867586135459`
        // Nope they were right in the last digit, too.
        // Hmph!  All right I guess I should use their formulation?

        // But WAIT a minute...
        // if acosh1p(u=x-1) is 2 * asinh(Math.sqrt(u/2))
        //    acosh(x) = acosh1p(x-1) = 2*asinh(sqrt((x-1)/2))
        // Yes!  In python:
        //    x = 1+1e-10
        //    from math import *
        //    2*asinh(sqrt((x-1)/2))
        // So: cosh1p(x) = 2*asinh(sqrt(x/2.))
        // So: cosh(x) = cosh1p(x-1) = 2*asinh(sqrt((x-1)/2.))
        // So, could use this, and it's just as accurate I think...
        // However I think it does one more sqrt than their formulation does.

        // OLD-- less good!
        // return 2 * Math.log(Math.sqrt((x+1)*.5) + Math.sqrt((x-1)*.5));

        // NEW-- better!
        // XXX TODO: add confidence test verifying it's very well behaved near 1, as shown above
        return acosh1p(x-1.);
    }

    /** inverse hyperbolic tangent function */
    public static double
    atanh(double x)
    {
        // Only defined for x < 1.

        // atanh(x) = log((1+x)/(1-x))/2
        //          = log((1 - x + 2x) / (1-x)) / 2
        //          = log(1 + 2x/(1-x)) / 2
        //          = log1p(2x/(1-x)) / 2
        // XXX https://code.google.com/p/v8/issues/detail?id=3266#c4  gives: 1/2*log1p(-2*x/(1+x))
        // XXX is that the same?? confused now.
        // XXX their derivation is:
        //      atanh(x) = 1/2*log((1-x)/(1+x))
        //               = 1/2 * log(1-2*x/(1+x))
        //               = 1/2*log1p(-2*x/(1+x))
        // But... is (1-x)/(1+x) equal to 1-2*x/(1+x) ?
        // oh wait!  did I mess up?
        // Oh! I see, mine is right in that it's log((1+x)/(1-x))/2 .
        // I think his is wrong since he's got (1-x)/(1+x) instead of (1+x)/(1-x).
        // (I added a comment about that).
        // Q: he conjectures log1p(x)/2-log1p(-x)/2 is more accurate than .5 * log1p(2.*x/(1.-x)) ... is that true?  I doubt it.
        // gnuplot:
        //      log1p(x) = log(1.+x) - ((1.+x-1.)-x)/(1.+x)
        //      plot [-1e-15:1e-15] log(1.+x), log1p(x)
        //      plot [-4e-305:4e-305] log(1.+x), log1p(x)
        //      atanh_1(x) = .5 * (log1p(x) - log1p(-x))
        //      atanh_2(x) = .5 * log1p(2*x/(1.-x))
        //      plot [-1e-15:1e-15] atanh(x), atanh_1(x)*1.1, atanh_2(x)*1.2
        // Can I argue:
        //      atanh_1 is subtracting two things, so it must be less accurate?
        //      especially when the two things being subtracted are nearly equal and large compared to zero?
        // note atanh is only defined strictly in (-1,1)...
        // Huh!  Here is some evidence for atan_1 being more accurate:
        //      EXACT(x) = sprintf("%.17g",x)
        //      print EXACT(atanh(1e-10)), ' ', EXACT(atanh_1(1e-10)), ' ', EXACT(atanh_2(1e-10))
        //        1.000000082540371e-10 1e-10 9.9999999999999991e-11
        // oh but wait-- are we sure 1e-10 is in fact the right answer???
        // Well, if ask wolframalpha for exactly atanh(1e-10),
        // then we get 1.000 ... (20 0's)...0003333333333 something something...
        // so I guess that's some evidence... but still not conclusive!
        // Q: how do I express (double)(1e-10) to wolframalpha?
        // (and I guess that's a general question about how to express arbitrary doubles)
        //     `1e-10 * (1<<34)`
        //     `1e-10 * (1<<34) - 1`
        // Recall analysis above:
        //                Well... to start with, in python, let's get a mathematically exact
        //                representation of (double)(1+1e-10):
        //                `(1+1e-10 - 1) * (1<<49)`
        //                '56295.0'
        //                So that says that (double)(1+1e-10) is exactly 1+56295/2^49.
        // So how do we get (double)1e-10 exactly?
        // I.e. to exactly 53 bits of precision... or so.
        //     `1e-10 * (1<<34)`
        //         '1.7179869184'
        //     `1e-10 * (1<<33)`
        //         '0.8589934592'
        // Then how many further, to get an integer?
        //     `1e-10 * (1<<33) * (1<<53)`
        //         '7737125245533627.0'
        // Okay that says that 1e-10 * (1<<33) is exactly:
        //      7737125245533627 / (1<<53)
        //      7737125245533627 / 77371252455336267181195264
        // So 1e-10 is exactly:
        //      = 7737125245533627 / 2^86
        //      = 1.00000000000000003643219731549...  (16 zeros)
        // And wolframalpha says:
        //     atanh(7737125245533627 / 2^86) = 1.000...(16 0's)...000364355306e-10
        //                               = 1.0000000000000000364355306e-10
        // which is indeed (double)1e-10 :-(
        // Dammit!  I think that does give evidence that
        // the one we are using is not as good!?  That sucks!
        // Surely I can still make a counterargument?
        // Like, they both suck, atanh_1 *happens* to give a better
        // answer for this one, but atanh_2 gives a better answer
        // for others.
        // Can we actually do an error analysis?
        // Also what are adjacent doubles?
        //       7737125245533626 / 2^86 = 9.99999999999999907185...e-11  (yeah that was my answer)
        //       7737125245533627 / 2^86 = 1.00000000000000003643219731549...e-10
        //       7737125245533628 / 2^86 = 1.0000000000000001656791680...e-10
        // mathematically correct answer = 1.0000000000000000364355306...e-10
        // bleah, so actually they hit it spot-on... *still* I want to argue it was by luck!!
        //     q(x) = sprintf('%.17g %.17g %.17g %.17g', x, atanh(x), atanh_1(x), atanh_2(x))
        // oh fooey, it isn't looking good...
        // atanh_1 always seems to get it spot on; atanh_2 either gets it spot on or is off by 1 ulp
        // (and it can be either too low or too high):
        //     gnuplot> print q(1e-10)
        //         1e-10 1.000000082540371e-10 1e-10 9.9999999999999991e-11
        //     gnuplot> print q(1e-12)
        //         9.9999999999999998e-13 1.0000333894291097e-12 9.9999999999999998e-13 9.9999999999999978e-13
        //     gnuplot> print q(1e-13)
        //         1e-13 9.9975583367475354e-14 1e-13 9.999999999999999e-14
        //     gnuplot> print q(1e-14)
        //         1e-14 9.9920072216262085e-15 1e-14 1.0000000000000002e-14
        // dammit!
        // wait wtf; my atanh_2 seems to systematically wrong?  why??
        //     Q = 1e-10
        //     q = 1e-25
        //     plot [Q-q:Q+q] x, atanh_1(x), atanh_2(x)
        //     q = 1e-15
        //     set samples 100001
        //     plot [Q-q:Q+q] atanh_1(x)-x, atanh_2(x)-x
        // That gives the picture.
        // So I guess atanh_2 really does screw up more often :-( bleah!
        // Well, atanh is:
        //      x + x^3/3 + x^5/5 + x^7/7 + x^9/9 + ...
        // Still wondering about the error analysis
        // and if there's an intuitive explanation.
        // Hmm is it that the two calls' average errors cancel out?
        // No that doesn't explain it, it seems like the *worst* case error will be *more*
        // than the worst case error from atanh_2, won't it?
        // Hmm can I find an example of that??
        // This is actually a pretty wacky example... the function's behavior
        // definitely aliases with the floating point representation,
        // and the proof may involve that.
        // Hmm let's break down what the successful one actually is:
        //      log1p(x) = log(1.+x) - ((1.+x-1.)-x)/(1.+x)
        //      atanh_1(x) = .5 * (log1p(x) - log1p(-x))
        // so that's:
        //      .5 * ((log(1+x) - ((x+1-1-x)/(1.+x)) - (log(1-x) -   (-x+1-1+x)/(1.-x))
        // huh, not helpful.
        return .5 * log1p(2.*x/(1.-x));
    }

    /** 1-cos(x), doesn't lose accuracy for small x */
    public static double
    cosf1(double x)
    {
        double sinHalfX = Math.sin(.5*x);
        return 2.*sinHalfX*sinHalfX;
    }
    /** acos(1-x), doesn't lose accuracy for small x */
    public static double
    acos1m(double x)
    {
        return 2.*Math.asin(Math.sqrt(.5*x));
    }

    /** sin(x)/x, but stable when small; same as sin_over_x(x). TODO: sincm1? */
    public static double
    sinc(double x)
    {
        //
        // It's 1 - x^2/3! + x^4/5! - ...
        // so if |x| is so small that 1-x^2/6 is indistinguishable from 1,
        // then the result will be indistinguishable from 1 too.
        //
        if (1. - x*x*(1/6.) == 1.)
        {
            //System.out.println("Ha! sin(x)/x("+x+") returning 1. instead of "+Math.sin(x)+"/"+x);
            return 1.;
        }
        else if (x == Double.NEGATIVE_INFINITY
              || x == Double.POSITIVE_INFINITY)
            return 0.;
        else
            return Math.sin(x)/x;
    }

    /** sin(x)/x, but stable when small; same as sinc(x). */
    public static double
    sin_over_x(double x)
    {
        return sinc(x);
    }

    /** sinh(x)/x, but stable when small */
    public static double
    sinh_over_x(double x)
    {
        //
        // It's 1 + x^2/3! + x^4/5! + ...
        //
        if (1. + x*x*(1/3.) == 1.)
        {
            //System.out.println("Ha! sinh(x)/x("+x+") returning 1. instead of "+Math.sin(x)+"/"+x);
            return 1.;
        }
        else if (x == Double.NEGATIVE_INFINITY
              || x == Double.POSITIVE_INFINITY)
            return Double.POSITIVE_INFINITY;
        else
            return sinh(x)/x;
    }

    /** asin(x)/x, but stable when small */
    public static double
    asin_over_x(double x)
    {
        // 1 + 1/6*x^2 + 3/40*x^4 + 5/112*x^6 + 35/1152*x^8 + 63/2816*x^10 + 231/13312*x^12 + ...
        if (1. + x*x*(1/3.) == 1.)
        {
            //System.out.println("Ha! asin(x)/x("+x+") returning 1. instead of "+MyMath.asin(x)+"/"+x);
            return 1.;
        }
        else
            return Math.asin(x)/x;
    }

    /** asinh(x)/x, but stable when small */
    public static double
    asinh_over_x(double x)
    {
        //
        // 1 - 1/6*x^2 + 3/40*x^4 - 5/112*x^6 + ...
        //
        if (1. - x*x*(1/6.) == 1.)
        {
            //System.out.println("Ha! asinh(x)/x("+x+") returning 1. instead of "+MyMath.asinh(x)+"/"+x);
            return 1.;
        }
        else if (x == Double.NEGATIVE_INFINITY
              || x == Double.POSITIVE_INFINITY)
            return 0.;
        else
            return sinh(x)/x;
    }

    /** (cosh(x)-1)/x^2, but stable when small */
    public static double
    cosf1_over_xx(double x)
    {
        double temp = sin_over_x(x*.5);
        return .5 * temp * temp;
    }

    /** (cosh(x)-1)/x^2, but stable when small */
    public static double
    coshm1_over_xx(double x)
    {
        double temp = sinh_over_x(x*.5);
        return .5 * temp * temp;
    }

    /** sqrt(x*x + y*y) but overflow/underflow proof. */
    public static double hypot(double x, double y)
    {
        x = Math.abs(x);
        y = Math.abs(y);
        double min, max;
        if (x < y)
        {
            min = x;
            max = y;
        }
        else
        {
            min = y;
            max = x;
        }
        if (min == 0.)
            return max;
        double min_over_max = min/max;
        return max * Math.sqrt(1. + min_over_max*min_over_max);
    }

    /** Lambert W function, that is, functional inverse of x = f(w) = w e^w.
    * http://blogs.mathworks.com/cleve/2013/09/02/the-lambert-w-function/
    * Copyright 2013 The MathWorks, Inc.
    */
    public static double lambertw(double x,
                                  int branch) // 0 means primary or upper, -1 means lower
    {
        if (Double.isNaN(x))
            return Double.NaN;
        double w;
        if (branch == 0)
            w = 1; // start above -1
        else if (branch == -1)
            w = -2; // start below -1
        else
            throw new IllegalArgumentException("MyMath.lambertw called with illegal branch "+branch);

        if (x <= -1./Math.E)
        {
            if (x < -1./Math.E)
                return Double.NaN; // either branch
            else
                return -1.; // so we get this one exact, on either branch; it would be pretty sloppy otherwise
        }
        if (branch == -1)
        {
            if (x > 0.)
                return Double.NaN;
            else if (x == 0.)
                return Double.NEGATIVE_INFINITY;
            // else strictly negative, so has a solution
        }
        else // if branch == 0
        {
            if (x == Double.POSITIVE_INFINITY)
                return Double.POSITIVE_INFINITY;
        }

        double v = Double.POSITIVE_INFINITY * w;
        while (Math.abs((w - v)/w) > 1e-8) // XXX do something different here? not sure
        //while (Math.abs((w - v)/w) > 0.) // XXX do something different here I think
        {
            v = w;
            double e = Math.exp(w);
            double f = w*e - x; // Iterate to make this quantity zero
            w -= f/(e*(w+1.) - (w+2.)*f/(2.*w+2.));
        }
        return w;
    }

    /**
    * Functional inverse of e^x/x.
    * http://www1.american.edu/cas/mathstat/People/kalman/pdffiles/glog.pdf
    * Wolfram Alpha says this -W(-1/x)
    * and empirically, it uses the opposite branch from the one we got.
    * Subtle point: if x is 0 or -0 and branch is -1, we return -infinity.
    * (another possibly valid choice would be -0->-infinity, +0->nan)
    */
    public static double glog(double x,
                              int branch)
    {
        // This is a subtle decision, but on branch -1,
        // I think we want to return -infinity if x is either 0 or -0.
        if (x == 0.)
            x = -0.;  // so that we call lambertw on infinity rather than -infinity!

        int otherBranch = -1 - branch;
        return -lambertw(-1./x, -1-branch);
    }


    /* XXX just for unit testing sqr_difference. should probably make private, and access it through more obscure getDeclaredMethod() or make it a special case*/
    public static double sqr(double x) { return x*x; }
    public static double cub(double x) { return x*x*x; }

    private static void test_difference_function_2args(java.lang.reflect.Method method, java.lang.reflect.Method mathMethod, double T, double t)
    {
        System.out.println("            in test_difference_function_2args(T="+T+", t="+t+")");
        double tol = 1e-12;
        double answer, fT, fTplust;
        {
            try
            {
                // stuff using varargs invoke probably needs backporting
                answer = (double)method.invoke(null, T, t);
                fT = (double)mathMethod.invoke(null, T);
                fTplust = (double)mathMethod.invoke(null, T+t);
            }
            catch (IllegalArgumentException e) { assert(false); }
            catch (IllegalAccessException e) { assert(false); }
            catch (java.lang.reflect.InvocationTargetException e) { PRINT(e.getCause()); assert(false); }
        }
        if (Double.isNaN(answer) && Double.isNaN(fTplust-fT))
        {
            System.out.println("              good! (both NaN)");
        }
        else
        {
            assert_almost_eq(answer, fTplust-fT, tol);
            System.out.println("              good!");
        }
        System.out.println("            out test_difference_function_2args(T="+T+", t="+t+")");
    } // test_difference_function_2args
    private static void test_difference_function_3args(java.lang.reflect.Method method, java.lang.reflect.Method mathMethod, double Tn, double Td, double t)
    {
        System.out.println("            in test_difference_function_3args(T="+Tn+"/"+Td+", t="+t+")");
        double tol = 1e-12;
        double answer, fT, fTplust;
        {
            try
            {
                // stuff using varargs invoke probably needs backporting
                answer = (double)method.invoke(null, Tn, Td, t);
                fT = (double)mathMethod.invoke(null, Tn/Td);
                fTplust = (double)mathMethod.invoke(null, Tn/Td+t);
            }
            catch (IllegalArgumentException e) { assert(false); }
            catch (IllegalAccessException e) { assert(false); }
            catch (java.lang.reflect.InvocationTargetException e) { PRINT(e.getCause()); assert(false); }
        }
        if (Double.isNaN(answer) && Double.isNaN(fTplust-fT))
        {
            System.out.println("              good! (both NaN)");
        }
        else
        {
            if (true)
            {
                PRINT(fT);
                PRINT(fTplust);
                PRINT(fT-fTplust);
                PRINT(answer);
            }

            assert_almost_eq(answer, fTplust-fT, tol);
            System.out.println("              good!");
        }

        System.out.println("            out test_difference_function_3args(T="+Tn+"/"+Td+", t="+t+")");
    } // test_difference_function_3args

    public static void main(String args[])
    {
        System.out.println("    in MyMath.main");
        PRINT(tanh(Double.POSITIVE_INFINITY));
        PRINT(tanh(Double.NEGATIVE_INFINITY));

        if (true)
        {
            // Difference tests.
            java.lang.reflect.Method[] methods = MyMath.class.getMethods();
            for (int i = 0; i < methods.length; ++i)
            {
                java.lang.reflect.Method method = methods[i];
                String name = method.getName();
                if (!name.endsWith("_difference"))
                    continue;
                Class[] signature = method.getParameterTypes();
                System.out.println("    Testing "+name+" ("+signature.length+" args)");
                String prefix_function_name = name.substring(0, name.length()-"_difference".length());
                System.out.println("        prefix_function_name = "+prefix_function_name);
                // Find prefix method, in MyMath or Math.
                // Some are in both (e.g. cosh,sinh,tanh, in java 1.5 and later).
                Class mathClasses[] = {Math.class, MyMath.class};
                int nFound = 0;
                for (int iMathClass = 0; iMathClass < mathClasses.length; ++iMathClass)
                {
                    Class mathClass = mathClasses[iMathClass];
                    java.lang.reflect.Method mathMethod = null;
                    try
                    {
                        mathMethod = mathClass.getMethod(prefix_function_name, new Class[]{double.class});
                    }
                    catch (NoSuchMethodException e)
                    {
                        System.out.println("        no method "+prefix_function_name+" in "+mathClass.getName());
                        continue;
                    }
                    nFound++;
                    System.out.println("        found "+mathMethod.getName()+" in "+mathClass.getName());
                    if (signature.length == 2)
                    {
                        double Ts[] = {0., .1, .4, .5, .7, Math.sqrt(.5), .8, .9, 1., 1.1, Math.sqrt(2.)};
                        double ts[] = {-.1, 0., .1, .2};
                        for (int iT = 0; iT < Ts.length; ++iT)
                        for (int it = 0; it < ts.length; ++it)
                        {
                            double T = Ts[iT];
                            double t = ts[it];
                            test_difference_function_2args(method, mathMethod, T, t);
                        }
                    }
                    else if (signature.length == 3)
                    {
                        double Tns[] = {0., .3, .5, .7, 1., 1.5, 2.};
                        double Tds[] = {0., .3, .5, .7, 1., 1.5, 2.};
                        double ts[] = {-.1, 0., .1, .2};

                        for (int iTn = 0; iTn < Tns.length; ++iTn)
                        for (int iTd = 0; iTd < Tds.length; ++iTd)
                        for (int it = 0; it < ts.length; ++it)
                        {
                            double Tn = Tns[iTn];
                            double Td = Tds[iTd];
                            double t = ts[it];
                            if (Tn==0. && Td==0.)
                               continue;
                            if (name.startsWith("cosh")
                             && ((Tn>=0&&Td>=0&&Tn<Td) // T < 1.
                              || (Tn+t*Td<Td)))        // T+t<1 i.e. Tn/Td+t<1 i.e. Tn+t*Td<Td
                                continue;
                            test_difference_function_3args(method, mathMethod, Tn, Td, t);
                        }
                    }
                    else
                        assert(false);
                }
                assert(nFound > 0);
            }
        }

        assert_eq(Math.exp(Double.NEGATIVE_INFINITY), 0.);
        assert_eq(Math.exp(-1.), 1./Math.E);
        assert_eq(Math.exp(0.), 1.);
        assert_eq(Math.exp(1.), Math.E);
        assert_eq(Math.exp(Double.POSITIVE_INFINITY), Double.POSITIVE_INFINITY);

        assert_nan(Math.log(Double.NEGATIVE_INFINITY));
        assert_eq(Math.log(0.), Double.NEGATIVE_INFINITY);
        assert_eq(Math.log(1./Math.E), -1.);
        assert_eq(Math.log(1.), 0.);
        assert_eq(Math.log(Math.E), 1.);
        assert_eq(Math.log(Double.POSITIVE_INFINITY), Double.POSITIVE_INFINITY);

        assert_eq(MyMath.expm1(Double.NEGATIVE_INFINITY), -1.);
        assert_eq(MyMath.expm1(0.), 0.);
        assert_eq(MyMath.expm1(Double.POSITIVE_INFINITY), Double.POSITIVE_INFINITY);

        assert_nan(MyMath.log1p(Double.NEGATIVE_INFINITY));
        assert_eq(MyMath.log1p(-1.), Double.NEGATIVE_INFINITY);
        assert_eq(MyMath.log1p(0.), 0.);
        assert_eq(MyMath.log1p(Double.POSITIVE_INFINITY), Double.POSITIVE_INFINITY);

        assert_eq(MyMath.sinh(Double.NEGATIVE_INFINITY), Double.NEGATIVE_INFINITY);
        assert(MyMath.sinh(-.1) < 0.);
        assert_eq(MyMath.sinh(0.), 0.);
        assert_eq(MyMath.sinh(.1), -MyMath.sinh(-.1));
        assert_eq(MyMath.sinh(Double.POSITIVE_INFINITY), Double.POSITIVE_INFINITY);

        assert_eq(MyMath.cosh(Double.NEGATIVE_INFINITY), Double.POSITIVE_INFINITY);
        assert(MyMath.cosh(-.1) > 1.);
        assert_eq(MyMath.cosh(0.), 1.);
        assert_eq(MyMath.cosh(.1), MyMath.cosh(-.1));
        assert_eq(MyMath.cosh(Double.POSITIVE_INFINITY), Double.POSITIVE_INFINITY);

        assert_eq(MyMath.tanh(Double.NEGATIVE_INFINITY), -1.);
        assert(MyMath.tanh(-.1) < 0.);
        assert_eq(MyMath.tanh(0.), 0.);
        assert_eq(MyMath.tanh(.1), -MyMath.tanh(-.1));
        assert_eq(MyMath.tanh(Double.POSITIVE_INFINITY), 1.);


        assert_eq(MyMath.asinh(Double.NEGATIVE_INFINITY), Double.NEGATIVE_INFINITY);
        assert(MyMath.asinh(-.1) < 0.);
        assert_eq(MyMath.asinh(0.), 0.);
        assert_eq(MyMath.asinh(.1), -MyMath.asinh(-.1));
        assert_eq(MyMath.asinh(Double.POSITIVE_INFINITY), Double.POSITIVE_INFINITY);

        assert_nan(MyMath.acosh(Double.NEGATIVE_INFINITY));
        assert_nan(MyMath.acosh(-1.));
        assert_nan(MyMath.acosh(0.));
        assert_nan(MyMath.acosh(.9));
        assert_eq(MyMath.acosh(1.), 0.);
        assert_eq(MyMath.acosh(Double.POSITIVE_INFINITY), Double.POSITIVE_INFINITY);

        assert_nan(MyMath.atanh(Double.NEGATIVE_INFINITY));
        assert_nan(MyMath.atanh(-1.1));
        assert_eq(MyMath.atanh(-1.), Double.NEGATIVE_INFINITY);
        assert(MyMath.atanh(-.1) < 0.);
        assert_eq(MyMath.atanh(0.), 0.);
        assert_eq(MyMath.atanh(.1), -MyMath.atanh(-.1));
        assert_eq(MyMath.atanh(1.), Double.POSITIVE_INFINITY);
        assert_nan(MyMath.atanh(1.1));
        assert_nan(MyMath.atanh(Double.POSITIVE_INFINITY));


        assert_eq(MyMath.coshm1(Double.NEGATIVE_INFINITY), Double.POSITIVE_INFINITY);
        assert(MyMath.coshm1(-.1) > 0.);
        assert_eq(MyMath.coshm1(0.), 0.);
        assert_eq(MyMath.coshm1(.1), MyMath.coshm1(-.1));
        assert_eq(MyMath.coshm1(Double.POSITIVE_INFINITY), Double.POSITIVE_INFINITY);

        assert_nan(MyMath.acosh1p(Double.NEGATIVE_INFINITY));
        assert_nan(MyMath.acosh1p(-.1));
        assert_eq(MyMath.acosh1p(0.), 0.);
        assert(MyMath.acosh1p(.1) > 0.);
        assert_eq(MyMath.acosh1p(Double.POSITIVE_INFINITY), Double.POSITIVE_INFINITY);


        assert_eq(MyMath.sin_over_x(Double.NEGATIVE_INFINITY), 0.);
        assert_eq(MyMath.sinh_over_x(Double.NEGATIVE_INFINITY), Double.POSITIVE_INFINITY);
        assert_nan(MyMath.asin_over_x(Double.NEGATIVE_INFINITY));
        assert_eq(MyMath.asinh_over_x(Double.NEGATIVE_INFINITY), 0.);
        assert_eq(MyMath.cosf1_over_xx(Double.NEGATIVE_INFINITY), 0.);
        assert_eq(MyMath.coshm1_over_xx(Double.NEGATIVE_INFINITY), Double.POSITIVE_INFINITY);

        assert_eq(MyMath.sin_over_x(0.), 1.);
        assert_eq(MyMath.sinh_over_x(0.), 1.);
        assert_eq(MyMath.asin_over_x(0.), 1.);
        assert_eq(MyMath.asinh_over_x(0.), 1.);
        assert_eq(MyMath.cosf1_over_xx(0.), .5);
        assert_eq(MyMath.coshm1_over_xx(0.), .5);

        assert_eq(MyMath.sin_over_x(Double.POSITIVE_INFINITY), 0.);
        assert_eq(MyMath.sinh_over_x(Double.POSITIVE_INFINITY), Double.POSITIVE_INFINITY);
        assert_nan(MyMath.asin_over_x(Double.POSITIVE_INFINITY));
        assert_eq(MyMath.asinh_over_x(Double.POSITIVE_INFINITY), 0.);
        assert_eq(MyMath.cosf1_over_xx(Double.POSITIVE_INFINITY), 0.);
        assert_eq(MyMath.coshm1_over_xx(Double.POSITIVE_INFINITY), Double.POSITIVE_INFINITY);

        // TODO: all of the following is missing testing of all the *_over_x functions
        // TODO: all of the following is missing testing of asinh, probably lots more

        // first and last entries are big enough to overflow all the exp and hyperbolic trig functions
        double xs[] = {-1000, -100, -10, -1, -.1, 0, .1, 1, 10, 100, 1000};
        for (int i = 0; i < xs.length; ++i)
        {
            double x = xs[i];

            System.out.println("    exp("+x+") = "+Math.exp(x)+"");
            System.out.println("    expm1("+x+") = "+MyMath.expm1(x)+"");
            System.out.println("    sinh("+x+") = "+MyMath.sinh(x)+"");
            System.out.println("    cosh("+x+") = "+MyMath.cosh(x)+"");
            System.out.println("    coshm1("+x+") = "+MyMath.coshm1(x)+"");

            assert_eq(MyMath.expm1(x) + 1., Math.exp(x) - 1. + 1.);
            assert_eq(MyMath.expm1(x) + 1. - 1., Math.exp(x) - 1.);

            if (x > -1.)
            {
                System.out.println("    log1p("+x+") = "+MyMath.log1p(x)+"");
                // TODO: come up with something more sensible I think? I'm a bit confused.
                assert_eq((float)MyMath.log1p(x), (float)Math.log(1.+x));
            }

            if (x >= 0.)
            {
                System.out.println("    acosh1p("+x+") = "+MyMath.acosh1p(x)+"");
                // TODO: come up with something more sensible I think? I'm a bit confused.
                assert_eq((float)MyMath.acosh1p(x), (float)MyMath.acosh(1.+x));
                // TODO: since acosh is defined in terms of acosh1p, that didn't test much, in fact I can deliberately introduce errors in it and they don't get caught :-(
            }

            if (true)
            {
                // hmm, not quite exact, apparently
                // TODO: actually this succeeds now? figure out if I want it
                PRINT(MyMath.coshm1(x) + 1.);
                PRINT(MyMath.cosh(x) - 1. + 1.);
                assert_eq(MyMath.coshm1(x) + 1., MyMath.cosh(x) - 1. + 1.);
                PRINT(MyMath.coshm1(x) + 1. - 1.);
                PRINT(MyMath.cosh(x) - 1.);
                assert_eq(MyMath.coshm1(x) + 1. - 1., MyMath.cosh(x) - 1.);
            }
            else
            {
                // TODO: come up with something more sensible I think? I'm confused
                assert_eq((float)MyMath.coshm1(x), (float)(MyMath.cosh(x) - 1.));
            }

            if (x >= 0.)
            {
                assert_eq(MyMath.sinh(-x), -MyMath.sinh(x));
                assert_eq(MyMath.asinh(-x), -MyMath.asinh(x));
                assert_eq(MyMath.cosh(-x), MyMath.cosh(x));
            }

            if (i == 0)
            {
                assert_eq(Math.exp(x), 0.);
                assert_eq(MyMath.expm1(x), -1.);
                assert_eq(MyMath.sinh(x), Double.NEGATIVE_INFINITY);
                assert_eq(MyMath.cosh(x), Double.POSITIVE_INFINITY);
            }
            if (i == xs.length-1)
            {
                assert_eq(Math.exp(x), Double.POSITIVE_INFINITY);
                assert_eq(MyMath.expm1(x), Double.POSITIVE_INFINITY);
                assert_eq(MyMath.sinh(x), Double.POSITIVE_INFINITY);
                assert_eq(MyMath.cosh(x), Double.POSITIVE_INFINITY);
            }

            if (i > 0.)
            {
                assert(Math.exp(xs[i-1]) < Math.exp(xs[i]));
                assert(MyMath.expm1(xs[i-1]) <= MyMath.expm1(xs[i])); // not strict < for some
                assert(MyMath.sinh(xs[i-1]) < MyMath.sinh(xs[i]));
                if (ABS(xs[i-1]) < ABS(xs[i]))
                {
                    assert(MyMath.cosh(xs[i-1]) < MyMath.cosh(xs[i]));
                    assert(MyMath.coshm1(xs[i-1]) < MyMath.coshm1(xs[i]));
                }
                else
                {
                    assert(MyMath.cosh(xs[i-1]) > MyMath.cosh(xs[i]));
                    assert(MyMath.coshm1(xs[i-1]) > MyMath.coshm1(xs[i]));
                }
            }
        }

        if (true)
        {
            // lambertw tests
            for (double i = -714.; i <= 696.; ++i) // empirically, works to this range
            {
                if (i <= -1.)
                {
                    if (i >= -10 && i <= 10) System.out.println("    lambertw(e^"+i+"*"+i+", -1) = "+MyMath.lambertw(Math.exp(i)*i,-1));
                    assert_eq(MyMath.lambertw(Math.exp(i)*i,-1), i); // seems to be exact
                }
                if (i >= -1.)
                {
                    if (i >= -10 && i <= 10) System.out.println("    lambertw(e^"+i+"*"+i+", 0) = "+MyMath.lambertw(Math.exp(i)*i,0));
                    assert_eq(MyMath.lambertw(Math.exp(i)*i,0), i); // seems to be exact
                }
            }
            assert_eq(MyMath.lambertw(Double.POSITIVE_INFINITY, 0), Double.POSITIVE_INFINITY);
            assert_nan(MyMath.lambertw(Double.POSITIVE_INFINITY, -1));

            // wolframalpha says the inverse function of e^x/x is -W(-1/x)
            // and we have to figure out which branch it is.
            // then:
            //     x = -W(-1/y)
            // and empirically, it's the lower branch. Hmm.  (wolframalpha didn't indicate that)
            // XXX hmm, -14 and -13 (not as much) are slow! interesting! should investigate!
            // that's
            //     -14: lambert(-5.9394908507397704E-8)
            //     -13: -1.7387149284469648E-7
            //     -12: -5.120176961106842E-7
            //     -11: -1.5183364354768781E-6
            OUT("========");
            for (double i = -100.; i <= 100.; ++i)
            {
                int branch = i<1 ? -1 : 0;
                if (i >= -20 && i <= 20) System.out.println("    MyMath.glog(Math.exp("+i+")/"+i+"="+(Math.exp(i)/i)+","+branch+") = "+MyMath.glog(Math.exp(i)/i,branch));
                assert_eq(MyMath.glog(Math.exp(i)/i,branch), i);

                if (i < 0.)
                {
                    assert(MyMath.glog(i, -1) < 0.);
                    assert_nan(MyMath.glog(i, 0));
                }
                else if (i == 0)
                {
                    // subtle: it's implemented so that both -0 and 0 produce NEGATIVE_INFINITY on branch -1.
                    assert_eq(MyMath.glog(i, -1), Double.NEGATIVE_INFINITY);
                    assert_eq(MyMath.glog(-i, -1), Double.NEGATIVE_INFINITY);
                    assert_nan(MyMath.glog(i, 0));
                    assert_nan(MyMath.glog(-i, 0));
                }
                else if (i < Math.E)
                {
                    assert_nan(MyMath.glog(i, -1));
                    assert_nan(MyMath.glog(i, 0));
                }
                else if (i == Math.E)
                {
                    assert_eq(MyMath.glog(i, -1), 1.);
                    assert_eq(MyMath.glog(i, 0), 1.);
                }
                else if (i > Math.E)
                {
                    assert(MyMath.glog(i, -1) < 1.);
                    assert(MyMath.glog(i, 0) > 1.);
                }
            }
        } // lambertw and glog tests

        System.out.println("      MyMath functions all good!");
        System.out.println("    out MyMath.main");
    }
} // class MyMath
